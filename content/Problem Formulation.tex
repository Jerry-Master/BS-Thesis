\chapter{Problem Formulation and State Of The Art}

\section{Definition}

In order to estimate the percentage of tumoural cells we decided to solve other problem. Instead of just predicting a number from a WSI, which would make the problem a logistic regression problem, we decided to segment and classify every cell to later count them and compute the percentage. There are two reasons why we chose to do so. The first one is because it makes models more interpretable, and the second one is because it makes the problem easier to solve.

Developing more on that first reason, we work in the medical field. Here, having a model that is statistically better than, say, a student is not enough to consider it really better than the student. The student can explain itself on why it made that diagnosis and so it provides more insight on how and why it makes mistakes. A seemingly black box model that simply outputs a number (the percentage of tumoural cells) gives no insight whatsoever of how and when you may expect it to fail. This is very dangerous in a medical setting. If we were at a factory classifying packages, we don't care so much about that. We can recover lost packages later on while still benefiting from the efficiency of using a automated process. However, we cannot recover dead patients. That makes interpretability a must. If our model predicts cells we can see which cells are causing more trouble. This helps know which data is needed to used for retraining the model and fixing those mistakes in the future. In the end, we want a model that can learn from its mistakes, looking at individual cell predictions makes it easier to recognise patterns in the mistakes it is doing and so it makes it easier to incrementally improve the model from its errors.

The second reason is that the regression problem is harder to solve. It may seem to be the other way around but it is not. Having worked with machine learning problems for years, my experience is that a regression problem where images are the input is almost never a good idea. Reframing the problem to solve something apparently harder in between has brought me better results in the past. Nonetheless, let's describe mathematically each problem in order to analyse each of them and come to an intuition of why it is a bad idea.

First, let's denote by $\mathcal{X}$ and $\mathcal{Y}$ the input and output space of the problem. We expect to find a function $f : \mathcal{X} \to \mathcal{Y}$ that effectively predicts the correct percentage given the WSI. Any WSI is no more than a collection of pixels, for that reason they can be viewed as very high dimensional vectors $\mathcal{X} = \mathbb{R}^N$, with $N > 10^{9}$ \cite{DICOM}. Similarly, the percentage is just a number between 0 and 1, so $\mathcal{Y} = [0,1]$.

Now, the logistic regression approach consists of finding such function from a family of parametric models $\mathcal{F}_\theta = \{ f_\theta | f_\theta : \mathcal{X} \to \mathcal{Y} \}$ while the segmentation approach tries to find $f$ differently. It first divides the WSI into patches. Then, each patch is processed independently to obtain pixel-wise predictions which are then used to compute the percentage. So, the family of parametric models now in consideration is $\mathcal{G}_\theta = \{ g_\theta | g_\theta : \mathcal{X}_s \to \mathcal{X}_s,\ \mathcal{X}_s \subset \mathcal{X} \}$ where $\mathcal{X}_s$ represents the space of images of 1024 by 1024 pixels, meaning $\text{dim}(\mathcal{X}_s) \approx 3.1 \cdot 10^6$. Given any segmentation model $g_\theta$ we construct its regression counterpart $f_\theta$ as described in the following commutative diagram. 

\[ \begin{tikzcd}
\mathcal{X} \arrow{r}{f_\theta} \arrow[swap]{d}{s} & \mathcal{Y} \\%
\mathcal{X}_s^P \arrow{r}{\tilde{g}_\theta}& \mathcal{X}_s^P \arrow{u}{c}
\end{tikzcd}
\]

Where we have extended $g_\theta$ to several patches by applying it independently to all of them.

\begin{align}
\begin{split}
\tilde{g}_\theta : \mathcal{X}_s^P& \to \mathcal{X}_s^P\\
(x_1, ..., x_P)& \mapsto (g_\theta(x_1), ..., g_\theta(x_P))
\end{split}
\end{align}

And the $s$ and $c$ functions refer to the split and count operations. Given a WSI it is split into several patches, for each patch, every cell is segmented and then all the tumoural and healthy cells are counted.

Without making any further assumptions about the families of functions we can derive some important insights about the advantages and disadvantages of each approach. First of all, it is clear that in the first approach the models have access to a wider context. Considering independent patches limits the ability to take into account global information. For instance, in lung tissue there is a structure called the cilium. It is a filamentous structure that appears near bronchioles. Its own existence is reason enough for considering the nearby cells as healthy. An example of such structure is depicted on \autoref{fig:lung-cilium}. The presence of cilium in the middle of a WSI cannot be taken into account when classifying the border of the tissue. But this disadvantage is not such a big deal. In a 1024 by 1024 image there is room enough for all the cells affected by the cilium, so limiting to such patches is enough. There is also another type of cell that interacts with their surrounding, the erythrocyte, also called red blood cell. All the cells glued to it are considered healthy. But that only applies to very close cells to it, so by using patches you will have no problem with this type of interactions. On the other hand, if we look at the dimensionality of input and output spaces we see a clear difference between the two approaches. For $\mathcal{F}_\theta$ the input space has a huge dimensionality while the output is uni-dimensional. In contrast, for $\mathcal{G}_\theta$ the input and output space both have the same dimensionality which is several orders of magnitude smaller than the dimensionality of $\mathcal{X}$. This makes the first approach quite prone to the curse of dimensionality \cite{AnalyticsVidhya}\footnote{Notice that this is not true for other regression problems, NeRFs \cite{mildenhall2020nerf} need to increase the dimensionality of the input to work properly.}. Even worse, we have just one number per each WSI. Even a simple regression problem needs more than 30 samples to have a reasonable amount of uncertainty. Having such amount of WSI is a very difficult task.

One may also think that the regression problem can be split into patches too. That is, solving the problem for individual patches and then averaging the percentages. This way the difference in dimensionality of the input and output spaces is lower. Moreover, the data scarcity problem is solved. But it is not so easy. In my experience, even with 224 by 224 images, regression is quite unfeasible if done in a naive way. On the other hand, for the individual percentages of each patch to be enough we will need to label all the patches of a WSI. Otherwise the sampling may not be uniform enough since WSI are very different from one part of the image to another, in other words, they are highly non-stationary. Labelling all the patches of one WSI could mean labelling thousands of images, which is unfeasible.

All in all, we chose this way because we believed it was going to give better results and because the doctors preferred that solution over the other for its interpretability.

\section{Data}\label{sec:data}

Now that we have described the problem as a segmentation and classification one, let's describe the datasets in which it is going to be applied and how they were obtained. The first one and the main focus of the thesis is the DigiPatics lung dataset, it is called like this because it was created under the DigiPatics project \cite{DigiPatics2022} and contains patches of lung WSI. Then, I'll describe another dataset also focused on tumour detection but of another organ, the DigiPatics breast dataset. And at the end I'll explain two more dataset: CoNSeP \cite{hovernet} and MoNuSAC \cite{8880654}. They are two public datasets related to digital pathology and nuclei segmentation and classification as well. 

\subsection{DigiPatics lung dataset}\label{sec:data_lung}

This dataset contains 85 images of 1024 by 1024 pixels referring to Hematoxilin and Eoxin stained WSI. Every patch comes from a WSI of a patient with lung tumour and it is annotated pixel-wise, that is, every pixel is either 0, 1 or 2 depending on whether it belongs to the background, to a healthy cell or to a tumoural cell. To have a better understanding of the problem at hand, look at \autoref{fig:labels} below.

\begin{figure}[ht]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{imgs/seg.png}
    \caption{Segmentation}
    \label{fig:seg}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
    \includegraphics[width=\textwidth]{imgs/overlay.png}
    \caption{Classification}
    \label{fig:class}
  \end{subfigure}
  \caption{Visualisation of the kind of labels that are needed. For each cell it is required to have its contour, as seen in the left, together with their corresponding class as seen in the right. Blue means tumoural and green non-tumoural.}
  \label{fig:labels}
\end{figure}

In order to annotate those images it is first needed to choose which images to label. One can extract hundreds of patches from surgical biopsies made by thoracotomy, and even in needle biopsies it is possible to obtain more than two hundred patches. But labelling those patches is very expensive and so they have to be chosen carefully. One important aspect to take into account is the number of patients at consideration. One thousand labelled images are of no use if they come from only one patient. There is a huge variance across WSI of different patients. For that reason we decided to annotate images from nine different patients. The selection within patient was made based on structures that seem very different from each other so that the dataset captures as much variance as possible.

Once the images were chosen, the annotation can begin. However, labelling manually each image can take days for each image. That is why we followed a semi-automated approach. To alleviate the amount of work required, an iterative procedure was designed. In a first step made by David Anglada and Feliu Formosa, the max-tree \cite{maxtree} was used to create rough segmentations of 24 patches that were later reviewed and improved by the students. Those initial labels were used to train Hovernet \cite{hovernet}, explained in detail in \autoref{sec:vision}. Using that newly trained model, 20 more images were annotated and reviewed by me. Then, Hovernet was trained again using those 44 images and used to infer the GT of 41 more patches. Only after carefully correcting all the 85 patches, would the pathologist Irene Sansano Valero start reviewing the dataset. The whole process took around 100 hours of human labour, 20 hours of GPU computation and 15 hours of expert human labour.

As a side note, this dataset is enough for the research carried out here but is definitely not enough for a production setting. Nine patients is by no means a representative sample. And there are many structures not present in the patches selected. We made our best effort to annotate a dataset that would validate our approach. Having proved its validity it remains to extend the dataset and scale the models in order to achieve a production ready model.

\subsection{DigiPatics breast dataset}\label{sec:data_breast}

Another tumour related dataset is provided by the DigiPatics group. This one was annotated by David Anglada and supervised by pathologist Teresa Soler. It contains 141 images from 4 different patients that had breast cancer. The biopsies were stained with HER2 staining. This time the dataset contains 6 different classes counting the background. I will not dive into much details of this dataset and what represents every class, the only fact I would like to mention is that domain experts on this field and the engineers collaborating with them do not think that group structure is as important here as it is in the case of lung tumours. In fact, experiments described in \autoref{subsec:gnn-xgb} and \autoref{subsec:void-gnn} show that this is the case. In \autoref{fig:mama_ex} you can see an example of the type of images that are in this dataset.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{imgs/data/p1.png}
    \caption{Patient 1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{imgs/data/p2.png}
    \caption{Patient 2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{imgs/data/p3.png}
    \caption{Patient 3}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.24\textwidth}
    \includegraphics[width=\textwidth]{imgs/data/p4.png}
    \caption{Patient 4}
  \end{subfigure}
    \caption{Patches of the four patients that are in this database. Names are omitted for data privacy. As you can observe, images are visually very different from patient to patient even though the same staining is employed in all of them.}
    \label{fig:mama_ex}
\end{figure}

\subsection{CoNSeP dataset}\label{sec:data_consep}

The name means colorectal nuclear segmentation and phenotypes. In other words, it is about cell nuclei detection on colorectal adenocarcinoma WSIs. Yet another organ in which we are interested of automatically visualise the cells that are related to a tumour. It uses the same staining as the DigiPatics lung dataset: H\&E. The size of the images is similar: 1000x1000. And it also has many patients: 16 in total. The main difference with the other datasets so far is that it only contains 41 images. Another difference with the lung dataset is that is has 7 classes, without counting the background. Instead of classifying cells as tumour vs non-tumour the authors here decided to provide a more detailed analysis of each cell. In \autoref{fig:consep_ex} you have an example of what are the images it contains. We decided to use this dataset because it was the one used in the Hovernet article although we don't have too much insight on this type of data. The results of the experiments on this dataset seem to prove that graphs neural network are not a good choice for colorectal tissue. Nonetheless, more research is needed to confirm that hypothesis. It remains to ask experts if the group structure is important here and other approaches need to be carried out, like binarising the classes. Since in this thesis I could only talk with lung experts, I couldn't carry those experiments myself. It is left as future research.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{imgs/data/consep.png}
    \caption{Different examples of the CoNSeP dataset illustrating all the different cells that it contains. Image was taken from the original article \cite{hovernet}.}
    \label{fig:consep_ex}
\end{figure}

\subsection{MoNuSAC dataset}\label{sec:data_monusac}

As in previous subsection, the abbreviation has some meaning. It comes from Multi-Organ Nuclei Segmentation and Classification Challenge. This time it is not tumour related but it is still a nuclei segmentation and classification problem so we deemed it interesting to try our method here. The MoNuSAC database contains 294 images from 71 patients. There are several differences with respect to the other three datasets. The first one is the type of classes it has. They are not tumour related, instead cells are classified into four types: epithelial, lymphocite, macrophage and neutrophil. The second difference is the number of organs. In the previous datasets only one tumour was considered at a time. Here we have lung, breast, kidney and prostate. The third and last difference is that the WSI came from 37 hospitals instead of just one. Every WSI was selected from The Cancer Genome Data Portal\footnote{\url{https://portal.gdc.cancer.gov/} Accessed 15th May 2023} and were later labelled. Having such amount of data made this dataset quite interesting to use. It has more variety and many more images than all the datasets previously mentioned. Interestingly enough, using graph neural networks here gives better results than not using them. Experimental results will be discussed in more detailed on \autoref{sec:results}. One technical detail to remark is that images from this dataset have very different aspect ratios and sizes. To simplify the process we just upsampled or downsampled every image into having size 1024x1024 using Lanczos interpolation. An example of the images from this dataset is in \autoref{fig:monusac_ex}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{imgs/data/monusac.png}
    \caption{Image extracted from the MoNuSAC article \cite{8880654}. It depicts various images from each of the four organs used in the creation of it.}
    \label{fig:monusac_ex}
\end{figure}

\section{Computer vision algorithms}\label{sec:vision}

In this section I will provide a brief survey about the state of the art in segmentations problems and a detailed explanation of Hovernet \cite{hovernet} which is the model used in the first phase of our method. Prior to diving into specific neural network architectures I will make a quick recap into what a convolutional neural network is. The forward pass of a single neuron from a layer of a neural network can be described as

\begin{equation}
    a_j = f(\bw^{(j)} \bx + b_j) = f\Big( \big(\sum_{i=1}^n w_i^{(j)} x_i\big) +b_j ) \Big)
\end{equation}

\noindent where $w^{j}$ is the j-th row of the weight matrix of that layer $W$, $x_i$ is the i-th entry of the input and $b_j$ is the j-th entry of the bias of that layer\footnote{\url{https://atcold.github.io/pytorch-Deep-Learning/en/week02/02-3/} Accessed 15th May 2023}. Here $f$ is any non-linearity to give the network expressivity. Now, a convolutional layer is similar in that it also has neurons that are made of a linear operator applied to the input plus a bias and a non-linearity. However, the operator is different. For a normal neural network, also called multilayer perceptron, the lineal operator can be expressed as $(W \cdot \bx)^{(j)}$ being $\cdot$ the matrix multiplication and $(\cdot)^{(j)}$ denotes the j-th entry of the vector. For a convolutional neural network, the operator is the convolution $(W * \bx)^{(j)}$. In one dimension, if $W = (w_1, w_2, w_3)^T$ is a kernel of dimension 3, then the convolution can be expressed as a matrix multiplication like so\footnote{\url{https://atcold.github.io/pytorch-Deep-Learning/en/week04/04-1/} Accessed 15th May 2023}

\begin{equation}
    W * \bx = \begin{bmatrix}
        w_1 & w_2  & w_3  & 0 & 0 & 0 & 0&\cdots  &0\\
        0 & w_1  & w_2 & w_3  & 0&0&0&\cdots &0\\
        0 & 0 & w_1 & w_2  & w_3  & 0&0&\cdots &0\\
        0 & 0 & 0& w_1  & w_2  &w_3 &0&\cdots &0\\
        0 & 0 & 0& 0 & w_1  &w_2 &w_3 &\cdots &0\\
        \vdots&&\vdots&&\vdots&&\vdots&&\vdots
    \end{bmatrix}
    \begin{bmatrix}
        x_1\\
        x_2\\
        x_3\\
        x_4\\
        \vdots\\
        x_k\\
        \vdots\\
        x_n
    \end{bmatrix}
\end{equation}

To extend it to 2D data like images, we could still use this matrix definition but it would get quite cumbersome so, instead, a simple formula can be used to describe 2D convolutions.

\begin{equation}
    (W * \bx) ^{(m,n)} = \sum_{i=-\infty}^{\infty} \sum_{j=-\infty}^{\infty} w^{(m-i, n-j)} \cdot x^{(i,j)}
\end{equation}

\noindent where the superindex notation denotes pixel coordinates and the weight matrix is centred at $(0,0)$ and has compact support, same as the image, which also has compact support meaning it is only different than zero for a finite amount of coordinates. As you may have noticed, convolutions return images as output. Therefore, we can visualise what a convolutional neural network is doing by looking at the neuron activations, which in this context are called the channels instead of neurons. In \autoref{fig:conv_ex} there is an example of two possible filters that may be learned by a convolutional neural network.

\begin{figure}[H]
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{imgs/conv/lena.png}
    \caption{Original}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{imgs/conv/lena2.jpg}
    \caption{Laplacian}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{imgs/conv/lena3.png}
    \caption{Gaussian}
  \end{subfigure}
    \caption{Example of convolution filters. At the left we have a very classical image, called Lena. On the center there is the laplacian filter applied to it using a convolution and on the right there is the gaussian filter applied to it.}
    \label{fig:conv_ex}
\end{figure}

A single convolution is not expressive enough to solve segmentation problems. Normally, several layers are required, but having too many layers has the problem of vanishing and exploding gradient which is why residual connections are needed. That was one of the ideas behind the first deep learning attempt at biomedical segmentation made by Olaf Ronneberger et al. \cite{unet}. They proposed an encoder-decoder architecture as shown in \autoref{fig:unet}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{imgs/unet.png}
    \caption{Original U-net architecture. It was composed by convolutional layers, pooling layers and residual connections.}
    \label{fig:unet}
\end{figure}

That architecture was improved recently with the development of transformers \cite{transformer}. In 2021 Jieneng Chen et al. invented TransUNet \cite{transunet} and later on in 2022 Jeya Maria Jose Valanarasu et al. created UNeXt \cite{unext}. I will not dive into the specifics of those architectures but rather comment on their limitations and why we couldn't use them. The key limitation is their sample efficiency. Even though transformers are more sample efficient in reinforcement learning than previous deep learning methods \cite{micheli2023transformers}, they still require a fair amount of data. In Jeya Maria Jose Valanarasu et al. \cite{unext} they used two datasets of 2594 and 647 images respectively. That is at least an order of magnitude more than what we could obtain. The other option, TransUNet \cite{transunet}, was tried in a dataset with 3779 computer tomography (CT) images, yet too much for us. Apart from that, the problem they tackled was organ segmentation, which is quite different from cell segmentation. In order to achieve better sample efficiency, a method with specific inductive biases is needed.

As explained in \autoref{sec:data}, the max-tree \cite{maxtree} can give good results at detecting cell contours while using no data at all. The algorithm used morphological properties of the cells to distinguish them from the background. That set a precedent, morphological algorithms could help us reduce the amount of data needed. Another important morphological algorithm is the watershed \cite{watershed}. It is known for creating accurate contours if the energy landscape is properly defined. Hovernet \cite{hovernet} combines both the U-net architecture with the watershed algorithm to produce cell segmentations and classify them. An overview is on \autoref{fig:hovpipe}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{imgs/hovpipe.png}
    \caption{Overview of the Hovernet method. It has three branches that predict different maps derived from the GT. In a post-processing step all the maps are combined using the watershed algorithm with carefully designed energy landscapes and markers.}
    \label{fig:hovpipe}
\end{figure}

Hovernet employs the same encoder-decoder architecture as U-net but it combines three different decoders with only one shared encoder. Each of the three decoders is trained to infer a different property from the GT. The NP branch separates the cells from the background, ignoring their class. The HV branch predicts horizontal and vertical distances from each pixel to the nuclei of the nearest cell. And the TP branch predicts the GT as is. Everything is trained end-to-end under one single loss function, which is shown below

\begin{align}
    \mathcal{L} &= \lambda_{mse}^{HV}\mathcal{L}_{mse}^{HV} + \lambda_{msge}^{HV}\mathcal{L}_{msge}^{HV} + \lambda_{bce}^{NP}\mathcal{L}_{bce}^{NP} + \lambda_{dice}^{NP}\mathcal{L}_{dice}^{NP} + \lambda_{bce}^{TP}\mathcal{L}_{bce}^{TP} + \lambda_{dice}^{TP}\mathcal{L}_{dice}^{TP} \\
    &= \frac{\lambda_{mse}^{HV}}{B}\left( \sum_{i=0}^{B} \| H(x_i) - h_i \|_2^2 + \sum_{i=0}^{B} \| V(x_i) - v_i \|_2^2 \right) \\
    &+ \frac{\lambda_{msge}^{HV}}{B}\left( \sum_{i=0}^{B} \| \nabla H(x_i) - gh_i \|_2^2 + \sum_{i=0}^{B} \| \nabla V(x_i) - gv_i \|_2^2 \right) \\
    &+ \frac{\lambda_{bce}^{NP}}{B} \sum_{i=0}^B \sum_{j=0}^{D} (y_{i}^{NP})_j \log (NP(x_i)_j) \\
    &+ \frac{\lambda_{dice}^{NP}}{B} \sum_{i=0}^B \left(1 - \frac{\sum_{j=0}^D NP(x_i)_j (y_{i}^{NP})_j}{\sum_{j=0}^D (NP(x_i)_j)^2 + \sum_{j=0}^D ((y_{i}^{NP})_j)^2}\right)\\
    &+ \frac{\lambda_{bce}^{TP}}{B} \sum_{i=0}^B \sum_{j=0}^{D} (y_{i}^{TP})_j \log (TP(x_i)_j) \\
    &+ \frac{\lambda_{dice}^{TP}}{B} \sum_{i=0}^B \left(1 - \frac{\sum_{j=0}^D TP(x_i)_j (y_{i}^{TP})_j}{\sum_{j=0}^D (TP(x_i)_j)^2 + \sum_{j=0}^D ((y_{i}^{TP})_j)^2}\right)
\end{align}

\noindent where all the $\lambda$ are hyperparameters, the letter $y$ denotes GT in any form, $h$ and $v$ are horizontal and vertical GT maps, $gh$ and $gv$ are the gradients of horizontal and vertical maps, $D$ is the number of pixels in any image, $B$ is the batch size, $\| \cdot \|_2^2$ is the $L_2$ norm, $H(\cdot)$ and $V(\cdot)$ are the outputs of the HV branch, $NP(\cdot)$ the output of the NP branch and $TP(\cdot)$ the output of the TP branch. This loss is particularly interesting because it combines multiple ideas. It is a mix of classification, regression and segmentation losses. Moreover, it can be considered as a second order optimisation since it is using the mean square error over the gradients. On the other side it combines the cross-entropy which is designed specially for classification problems while optimising the Dice loss which is more or less like maximising the intersection of predicted and real cells, more on that on \autoref{subsec:dice}. It is expected that optimising all the different objectives while using a single encoder is going to make that encoder extract features useful for a variety of tasks, thus generalising better.

After the model is trained, it can be used for inference in addition with a post-processing phase which consists of the watershed algorithm. This particular watershed requires an energy landscape which defines the space where the flooding is made and a marker that contains the starting points to start the flooding. Both are defined below

\begin{align}
    E &= (1 - \mathcal{S}_m(\bX)) \odot NP(\bX) \\
    M &= \text{ReLU}(NP(\bX) - \mathcal{S}_m(\bX))
\end{align}

\noindent being $E$ the energy and $M$ the marker. In those equations $\bX$ refers to the input image, ReLU is the rectified linear unit \cite{relu}, $\odot$ is the element-wise multiplication, also referred to as Hadamard product, and $\mathcal{S}_m(\bX)$ is the thresholded gradient of the HV branch as expressed here

\begin{equation}
    \mathcal{S}_m(\bX) = \max(S_x * H(\bX), S_y * V(\bX))
\end{equation}

\noindent where $S_x$ and $S_y$ are Sobel filters \cite{sobel} and $*$ is the convolution operation. The whole process can be visualised in \autoref{fig:hovpipe}. The reason for using gradient filters over the HV branch is that if the prediction is perfect, then such gradients are exactly the NP branch. Therefore, if we apply a watershed of one over the other it is expected that one branch fills the errors of the other. That is not so simple in practice, and for this post processing algorithm to work both the HV branch and the NP branch need to be expanded and contracted using a thresholding function over a threshold that was carefully selected by the authors based on empirical results.

\section{Graph neural networks}\label{sec:gnn}

Having described the state of the art for computer vision, let's introduce the state of the art of graph neural networks as well. Graph neural networks are very similar to neural networks but the key difference is that the computational graph is different for every node and it varies depending on which nodes it is connected to. GNNs as used in this thesis generate an embedding which decodes all the relevant information of that node. Most of the techniques used with neural networks can be extended to GNNs as well, like dropout \cite{dropout}, batch normalisation \cite{batchnorm} or pooling layers \cite{graph_survey}. In fact, there are more than 50 different possible architectures \cite{graph_survey} and more than 300.000 possible configurations \cite{you2021design}. However I will focus mainly on two of the most popular layers: graph convolution and graph attention. Both can be used for node classification which is what we are interested about since we are going to treat cells as nodes. More on that description in \autoref{sec:descr}.

\subsection{Graph convolution}\label{sec:gcn}

This architecture was proposed by Thomas N. Kipf et al. \cite{graphconv} in 2016 and has been cited almost ten thousands times as of this date. The main idea is to adapt the notion of convolution from images to graphs. An illustration of the concept can be seen in \autoref{fig:conv_comp}.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{imgs/conv.png}
        \caption{2D Convolution where each pixel can be considered a node connected to all its adjacent pixels. This operation returns the weighted average of adjacent pixels for each node.}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \includegraphics[width=\textwidth]{imgs/graph conv.png}
        \caption{Graph Convolution where the weighted average is taken with respect to adjacent nodes. There is no notion of pixels and each node has no absolute spatial coordinates.}
    \end{subfigure}
    \caption{Visualisation of 2D Convolution vs Graph Convolution taken from \cite{graph_survey}.}
    \label{fig:conv_comp}
\end{figure}

Mathematically, the graph convolution operation as expressed in \cite{graphconv} can be defined as shown below

\begin{equation}
    \bh_j^{(l+1)} = \sigma \left(\bb^{(l)} + \sum_{k \in \mathcal{N}_j} \frac{1}{c_{jk}} \bW^{(l)}\bh_k^{(l)}\right)
\end{equation}

\noindent where $\bb^{(l)}\in \R^d, \bW^{(l)} \in \R^{d\times d}$ are the bias and weights of the layer, $\mathcal{N}_j$ is the set of neighbours of node $j$, $c_{jk} = \sqrt{|\mathcal{N}_j|\cdot |\mathcal{N}_k|}$ is a normalisation factor and $\sigma$ is an activation function. The vectors $\bh_k^{(l)}$ are the hidden embeddings of the network for each layer, being $\bh_k^{(0)}$ an initial vector containing any relevant information about the node. That information can be the area of the cell, the average colour, or even a prior distribution for the class label. In the last layer, the weight matrix is of dimensions $C \times d$, where $C$ is the number of classes or $1$ if $C=2$ and the activation function is either the sigmoid for a binary problem or the softmax \cite{softmax} for a multi-class problem.

\subsection{Graph attention}

As an improvement over simply doing the average, one year after the publication of the graph convolution, Petar Veličković et al. \cite{graphatt} proposed the idea of including the attention mechanism \cite{attention} to compute a weighted average instead. This idea, which has been cited over eight thousands times, is visualised in \autoref{fig:gat}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{imgs/gat.png}
    \caption{On the left is the overview of the computation of the attention weights. For each adjacent node an attention weight is computed based on the similarity of their embeddings. On the right there is a visualisation about multi-head attention, which consists of concatenating the result of several attention mechanisms. The figures are taken from the original article \cite{graphatt}.}
    \label{fig:gat}
\end{figure}

More formally, the computation can be described as follows

\begin{equation}
    \bh_j^{(l+1)} = \sigma \left(\sum_{k \in \mathcal{N}_j} \alpha_{jk} \bW^{(l)}\bh_k^{(l)}\right)
\end{equation}

\noindent where $\bW^{l}\in \R^{d\times d}$ are the layer weights and $\alpha_{jk} \in \R$ are the attention weights which are defined by the following formula

\begin{equation}
    \alpha_{jk} = \frac{\exp(\text{LeakyReLU}(\ba \cdot [\bW \bh_j||\bW \bh_k]))}{\sum_{r\in \mathcal{N}_j} \exp(\text{LeakyReLU}(\ba \cdot [\bW \bh_j||\bW \bh_r]))}
\end{equation}

\noindent being $\ba \in \R^{2d'}, \bW \in \R^{d'\times d}$ two learnable projection matrices, LeakyReLU is the leaky rectified linear unit \cite{leakyrelu} and $||$ the concatenation operation. Inspired by the multi-head attention mechanism proposed in \cite{transformer}, the previous attention mechanism can be extended to $H$ heads

\begin{equation}
    \bh_j^{(l+1)} = \bigparallel_{h=1}^H \sigma \left(\sum_{k \in \mathcal{N}_j} \alpha_{jkh} \bW^{(l)}_h\bh_k^{(l)}\right)
\end{equation}

\noindent where now $\bW^{(l)}_h \in \R^{d \times Hd}$, the attention weights are different for each head and sum up to one in each head $\sum_{k\in\mathcal{N}_j}\alpha_{jkh}=1,\ \forall h$ and in the final layer heads are averaged instead of concatenated as explained in \cite{graphatt}. Notice that there is not bias as with the convolution. This is because the attention scores are thought to be similarities between nodes. The attention mechanism is just a way of averaging embeddings based on their similarity to the target node. There is no need for bias under that interpretation.

\section{XGBoost}

The last section of this chapter is devoted to give a very brief explanation of an algorithm that is going to be tangentially used in the experiments as a way to perform an ablation study to see if the graphs are really being useful or not. XGBoost \cite{xgboost} is an algorithm to perform either regression or classification over tabular data. In our case, it is going to be used over the extracted features of each cell (perimeter, area, ...) to predict its class. The XGBoost algorithm is in fact a gradient boosted tree, the distinction here comes because XGBoost is a faster implementation of such idea. To understand what a gradient boosted tree we first need to explain what a random forest is. It is a set of tree classifiers that are ensembled together using the average of their predictions. A visualisation of it is here below\footnote{\url{http://aiweb.techfak.uni-bielefeld.de/content/bworld-robot-control-software/} Accessed 25th January 2023}.

\scalebox{0.7}{
\centering
\begin{forest}
  for tree={l sep=3em, s sep=2em, anchor=center, inner sep=0.4em, fill=blue!50, circle, where level=2{no edge}{}}
  [
  Training Data, node box
  [sample and feature bagging, node box, alias=bagging, above=3em
  [,red!70,alias=a1[[,alias=a2][]][,red!70,edge label={node[above=1ex,red arrow]{}}[[][]][,red!70,edge label={node[above=1ex,red arrow]{}}[,red!70,edge label={node[below=1ex,red arrow]{}}][,alias=a3]]]]
  [,red!70,alias=b1[,red!70,edge label={node[below=1ex,red arrow]{}}[[,alias=b2][]][,red!70,edge label={node[above=1ex,red arrow]{}}]][[][[][,alias=b3]]]]
  [~~~$\dots$~,scale=2,no edge,fill=none,yshift=-3em]
  [,red!70,alias=c1[[,alias=c2][]][,red!70,edge label={node[above=1ex,red arrow]{}}[,red!70,edge label={node[above=1ex,red arrow]{}}[,alias=c3][,red!70,edge label={node[above=1ex,red arrow]{}}]][,alias=c4]]]]
  ]
  \node[tree box, fit=(a1)(a2)(a3)] (t1) {};
  \node[tree box, fit=(b1)(b2)(b3)] (t2) {};
  \node[tree box, fit=(c1)(c2)(c3)(c4)] (tn) {};
  \node[below right=0.5em, inner sep=0pt] at (t1.north west) {Tree 1};
  \node[below right=0.5em, inner sep=0pt] at (t2.north west) {Tree 2};
  \node[below right=0.5em, inner sep=0pt] at (tn.north west) {Tree $n$};
  \path (t1.south west)--(tn.south east) node[midway,below=4em, node box] (mean) {mean in regression or majority vote in classification};
  \node[below=3em of mean, node box] (pred) {prediction};
  \draw[black arrow={5mm}{4mm}] (bagging) -- (t1.north);
  \draw[black arrow] (bagging) -- (t2.north);
  \draw[black arrow={5mm}{4mm}] (bagging) -- (tn.north);
  \draw[black arrow={5mm}{5mm}] (t1.south) -- (mean);
  \draw[black arrow] (t2.south) -- (mean);
  \draw[black arrow={5mm}{5mm}] (tn.south) -- (mean);
  \draw[black arrow] (mean) -- (pred);
\end{forest}
}

Now, to pass from a random forest to a gradient boosted tree we have to substitute the idea of bagging (doing the average) to boosting. Boosting consists of incrementally add models to the ensemble that each of them predicts the error of the previous ensemble. This way every model you add operates on a smaller target variable. For classification problems, the residual is taken from the log likelihood so that it is treated as a regression problem over the real line. The process is represented on \autoref{fig:gdbt}\footnote{Image taken from \url{https://medium.com/analytics-vidhya/what-is-gradient-boosting-how-is-it-different-from-ada-boost-2d5ff5767cb2} Accessed 15th May 2023}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{imgs/conv/gdbt.png}
    \caption{Visualisation of the concept of boosting for tree models. Image taken from a medium post.}
    \label{fig:gdbt}
\end{figure}