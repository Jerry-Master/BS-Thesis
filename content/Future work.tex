\chapter{Future work}

The obvious lines of research here are improving the CNN backbone, improving the GNN head or training everything end-to-end instead of having two-phases as in this work. All of them would bring better metrics and more robust classification methods. However, the obvious is not always the best. Progress is made by impressive discoveries that few people expected. That is why I want to propose a different way of advancing science in this field. The classification paradigm has served us well, but a new paradigm is emerging nowadays. Multimodal models are on the rise. Image captioning models like BLIP \cite{li2022blip} or CLIP \cite{radford2021learning} have gained popularity in the last years. They provide a way of opening the black boxes that are neural networks. Captioning an image is just one step behind a model explaining itself and the reasoning about the result. Why not make a model explain cancer? Expert pathologist train new physicians showing them images and explaining them with voice or text. Why not train models the same way? Obviously those models won't be better than the experts, but they can outscale them. One person can only view a small amount of patients in a life. A model can be trained with billions of cases in months and can infer the result of millions of new cases in days given enough computational resources. So that is the non-obvious future work: Image captioning for medical image analysis.

I'll use the rest of the page to describe the roadmap to achieving the goal of medical images explaining themselves. First step: data. That is the most important part of the whole project. No data no models. Typically foundational models require billions of image-text pairs, or if only trained with text they require trillions of tokens. Such amount of medical data is not available yet, although it would be possible if experts were into it. But I'll assume physicians don't really want to spend time creating datasets (which is more or less my experience dealing with doctors). The good news is that one can fine-tune a foundational model with smaller amounts of data. With thousands of image-text pairs would be enough for a production ready model. To obtain such pairs pathologist would be required to think loud. That's it. We can automatically transcribe audio into text, so it is enough for them to record their voice while working. The next step is the model. In the previous paragraph I have shown two methods of jointly training a latent text-image space. But we can achieve much more than that, we want to speak to the images. Something like what is done in Mini-GPT4 \cite{zhu2023minigpt4}. Right now there is a wide variety of language models to be used as backbones. Stability AI recently released StableLM \footnote{\url{https://github.com/Stability-AI/StableLM}}, and we have the LLaMa family of models too \footnote{\url{https://ai.facebook.com/blog/large-language-model-llama-meta-ai/}}. We are not running out of open source LLMs anytime soon. Using them is a huge boost in performance and sample efficiency. Moreover, there is also DINOv2 \footnote{\url{https://dinov2.metademolab.com/}} which can be used as image encoder to further reduce the burden of training. We just need to merge everything and finetune it with the appropiate data. Sounds easy, right?