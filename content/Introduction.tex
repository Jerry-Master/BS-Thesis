\chapter{Introduction}
\section{When technology meets histology}

This project was born due to the necessity of alleviating physicians and researchers workload in the field of digital pathology. On a typical day, our experts have to go through a wide variety of tissue images in order to detect some anomaly or disease. Depending on the task at hand they sometimes need to estimate the proportion of tumoural cells with respect to healthy ones. How wonderful would it be if a machine could perform that task for them. That is the purpose of this thesis, to automatically estimate the percentage of tumoural cells with respect to non-tumoural ones and facilitate the physician's job.

Analysing human tissue is a challenging task. The first part of the process consists of extracting the tissue from the part of the body that is relevant to the patient's condition. If the patient is alive the extraction is typically done using a needle. Otherwise, if the organ is already removed then a cube of tissue can be sliced. Then, simply watching those slices through a microscope is not going to be enough. Cells are very small and its interior is difficult to observe even when looking through the lenses of a microscope. For that reason tissue is stained prior to observing it. There are different kind of staining, some of them highlight the cell nuclei, others the membrane and other stainings react with specific kind of cells. Another factor to take into account is the cost of the dye. Some of them make the task at hand easier but are too expensive to do for every patient. A trade-off is usually found where the more expensive one is used only when the cheaper is not enough for confidently diagnosing.

In the past, the surgically extracted slices were typically watched through the lenses of a microscope \cite{Chen2011}, what is termed as optical microscopy. With the development of new technologies, the field has become more and more digitalised \cite{Kumar2020}, now being called digital pathology. The resulting digital images that originate from watching through the microscope the tissue are called Whole Slide Images (WSI). Between when a biopsy procedure is made and when the specialist watches it, there is now a period of time required to digitalise it. In other words, at the morning one specialist carries out the removal of the tissue. Afterwards, technicians digitalise the image in the afternoon. It is in the next morning the physician watches the result. Taking advantage of that interval between when the image is digitised and when the doctor watches it, other computational methods can be applied prior to the experts receiving the images. This will enhance the physician user experience while working without the need for them to wait for the algorithm to give the result at real time because it is already precomputed. Having a preliminary diagnosis can help reduce the workload and make the histologist more productive.

\section{Why graphs?}

This whole thesis was initially thought to be about lung tissue. Starting from a WSI we are interested in detecting which cells are tumoural and which cells are healthy. The purpose of the application is to rapidly detect tissue slices that contain a high amount of tumoural DNA so that it can be later on processed and analysed. Finding such WSI requires the histologist to look at several of them and deciding which one to choose. So we want to provide a ranking of images from more likely of having a high percentage of tumour to less likely. This way, on average, the physician would require to look at less images per patient, making it possible to analyse more patients' WSI in the same time. So, where does graphs come into play?

In a previous thesis, the same problem was tackled using a computer vision-only approach \cite{upcommons353765}. Initially, I was asked to improve on that method. After several months working on the problem I began to notice the principal flaw (which is also the principal feature) of convolutional neural networks: an inductive bias towards locality \cite{DBLP:journals/corr/CohenS16a}. That means deep neural networks classify cells based on their immediate morphological properties. However, having met with pathologist Irene Sansano Valero, which is an expert histologist in the field of lung tumour, made it clear that cells were considered being tumoural or not depending on their surroundings. Visually identical cells may be classified differently if their neighbourhood is different. Here, I am always referring to lung tissue, we will later on analyse if this still holds true for other tissues. It was made clear then that a different approach was needed. Another kind of inductive bias was required, a relational inductive bias. One that classified cells based on their relationships with nearby cells and not only on their individual properties. This is the exact kind of inductive bias graph neural networks provide \cite{DBLP:journals/corr/abs-2104-13478}. 

This is how the idea came into existence. The exact details of how the graph is constructed and which networks are employed are later discussed. But the key idea is that if relations between cells are considered important when classifying them, using graphs in some way may lead to better results. We expect this hypothesis to be specially true for lung tissue and, in fact, we come across high evidence that it is so. The question then is, is it true for other tissues? The answer is it depends. Apart from the lung dataset we repeat all the experiments for other three datasets obtaining mixed results. Sometimes it is a good fit, sometimes it is not and sometimes it works better but not because it is a graph but because it is a stacking classifier. The exact meaning of these words and which experiments are made to prove it will be later discussed.